# Twitter
Twitterに関するもの

import sysabcdefimport cv2abcdefimport numpy as npabcdefimport tensorflow as tfabcdefimport tensorflow.python.platformabcdefimport osabcdefimport globabcdefimport scipyabcdeffrom scipy import ioabcdefimport timeabcdeffrom tensorflow.contrib import learnabcdefimport randomabcdefabcdef###############################################################################abcdef# Constants for the image input and output.abcdef###############################################################################abcdefabcdef# Output folder for the images.abcdefOUTPUT_DIR = 'output/'abcdef# Style image to use.abcdefSTYLE_IMAGE = 'images/guernica.jpg'abcdef# Content image to use.abcdefCONTENT_IMAGE = 'images/hongkong.jpg'abcdef# Image dimensions constants. abcdefIMAGE_WIDTH = 400abcdefIMAGE_HEIGHT = 400abcdefCOLOR_CHANNELS = 3abcdefabcdef###############################################################################abcdef# Algorithm constantsabcdef###############################################################################abcdef# Noise ratio. Percentage of weight of the noise for intermixing with theabcdef# content image.abcdefNOISE_RATIO = 0.6abcdefabcdefVGG_MODEL = 'imagenet-vgg-verydeep-19.mat'abcdefabcdefMEAN_VALUES = np.array([123.68, 116.779, 103.939]).reshape((1,1,1,3))abcdefabcdefdef build_model(input_img,IMAGE_WIDTH,IMAGE_HEIGHT,CHANNEL, layer_num):abcdefzzzdef conv_layer(layer_name, layer_input, W):abcdefzzzzzzconv = tf.nn.conv2d(layer_input, W, strides=[1, 1, 1, 1], padding='SAME')abcdefzzzzzzreturn convabcdefabcdefzzzdef relu_layer(layer_name, layer_input, b):abcdefzzzzzzrelu = tf.nn.relu(layer_input + b)abcdefzzzzzzreturn reluabcdefabcdefzzzdef pool_layer(layer_name, layer_input):abcdefzzzzzzpool = tf.nn.avg_pool(layer_input, ksize=[1, 2, 2, 1], abcdefzzzzzzzzzstrides=[1, 2, 2, 1], padding='SAME')abcdefzzzzzzzzz# pool = tf.nn.max_pool(layer_input, ksize=[1, 2, 2, 1], abcdefzzzzzzzzz#zzz strides=[1, 2, 2, 1], padding='SAME')abcdefzzzzzzreturn poolabcdefabcdefzzzdef get_weights(vgg_layers, i):abcdefzzzzzzweights = vgg_layers[i][0][0][2][0][0]abcdefzzzzzzW = tf.constant(weights)abcdefzzzzzzreturn Wabcdefabcdefzzzdef get_bias(vgg_layers, i):abcdefzzzzzzbias = vgg_layers[i][0][0][2][0][1]abcdefzzzzzzb = tf.constant(np.reshape(bias, (bias.size)))abcdefzzzzzzreturn babcdefabcdefzzznet = {}abcdefzzzabcdefzzzvgg_rawnetzzzzzz = scipy.io.loadmat(VGG_MODEL)abcdefzzzvgg_layerszzzzzz = vgg_rawnet['layers'][0]abcdefzzznet['input']zzz = tf.Variable(np.asarray([input_img]), dtype=np.float32)abcdef# np.zeros((1,IMAGE_WIDTH,IMAGE_HEIGHT,CHANNEL)abcdefzzzif layer_num==1:abcdefzzzzzznet['conv1_1'] = conv_layer('conv1_1', net['input'], W=get_weights(vgg_layers, 0))abcdefzzzzzznet['relu1_1'] = relu_layer('relu1_1', net['conv1_1'], b=get_bias(vgg_layers, 0))abcdefabcdefzzzzzznet['conv1_2'] = conv_layer('conv1_2', net['relu1_1'], W=get_weights(vgg_layers, 2))abcdefzzzzzznet['relu1_2'] = relu_layer('relu1_2', net['conv1_2'], b=get_bias(vgg_layers, 2))abcdefzzzzzzabcdefzzzzzznet['pool1']zzz = pool_layer('pool1', net['relu1_2'])abcdefabcdefzzzzzz# if args.verbose: print('LAYER GROUP 2')zzzabcdefzzzzzznet['conv2_1'] = conv_layer('conv2_1', net['pool1'], W=get_weights(vgg_layers, 5))abcdefzzzzzznet['relu2_1'] = relu_layer('relu2_1', net['conv2_1'], b=get_bias(vgg_layers, 5))abcdefabcdefzzzif layer_num == 2:zzzabcdefzzzzzznet['conv2_1'] = conv_layer('conv2_1', net['input'], W=get_weights(vgg_layers, 5))abcdefzzzzzznet['relu2_1'] = relu_layer('relu2_1', net['conv2_1'], b=get_bias(vgg_layers, 5))abcdefzzzzzznet['conv2_2'] = conv_layer('conv2_2', net['relu2_1'], W=get_weights(vgg_layers, 7))abcdefzzzzzznet['relu2_2'] = relu_layer('relu2_2', net['conv2_2'], b=get_bias(vgg_layers, 7))abcdefzzzzzzabcdefzzzzzznet['pool2']zzz = pool_layer('pool2', net['relu2_2'])abcdefzzzzzznet['conv3_1'] = conv_layer('conv3_1', net['pool2'], W=get_weights(vgg_layers, 10))abcdefzzzzzznet['relu3_1'] = relu_layer('relu3_1', net['conv3_1'], b=get_bias(vgg_layers, 10))abcdefabcdefzzzif layer_num == 3:abcdefzzzzzznet['conv3_1'] = conv_layer('conv3_1', net['input'], W=get_weights(vgg_layers, 10))abcdefzzzzzznet['relu3_1'] = relu_layer('relu3_1', net['conv3_1'], b=get_bias(vgg_layers, 10))abcdefabcdefzzzzzznet['conv3_2'] = conv_layer('conv3_2', net['relu3_1'], W=get_weights(vgg_layers, 12))abcdefzzzzzznet['relu3_2'] = relu_layer('relu3_2', net['conv3_2'], b=get_bias(vgg_layers, 12))abcdefabcdefzzzzzznet['conv3_3'] = conv_layer('conv3_3', net['relu3_2'], W=get_weights(vgg_layers, 14))abcdefzzzzzznet['relu3_3'] = relu_layer('relu3_3', net['conv3_3'], b=get_bias(vgg_layers, 14))abcdefabcdefzzzzzznet['conv3_4'] = conv_layer('conv3_4', net['relu3_3'], W=get_weights(vgg_layers, 16))abcdefzzzzzznet['relu3_4'] = relu_layer('relu3_4', net['conv3_4'], b=get_bias(vgg_layers, 16))abcdefabcdefzzzzzznet['pool3']zzz = pool_layer('pool3', net['relu3_4'])abcdefabcdefzzzzzz# if args.verbose: print('LAYER GROUP 4')abcdefzzzzzznet['conv4_1'] = conv_layer('conv4_1', net['pool3'], W=get_weights(vgg_layers, 19))abcdefzzzzzznet['relu4_1'] = relu_layer('relu4_1', net['conv4_1'], b=get_bias(vgg_layers, 19))abcdefabcdefzzzif layer_num == 4:abcdefzzzzzznet['conv4_1'] = conv_layer('conv4_1', net['input'], W=get_weights(vgg_layers, 19))abcdefzzzzzznet['relu4_1'] = relu_layer('relu4_1', net['conv4_1'], b=get_bias(vgg_layers, 19))abcdefabcdefzzzzzznet['conv4_2'] = conv_layer('conv4_2', net['relu4_1'], W=get_weights(vgg_layers, 21))abcdefzzzzzznet['relu4_2'] = relu_layer('relu4_2', net['conv4_2'], b=get_bias(vgg_layers, 21))abcdefabcdefzzzzzznet['conv4_3'] = conv_layer('conv4_3', net['relu4_2'], W=get_weights(vgg_layers, 23))abcdefzzzzzznet['relu4_3'] = relu_layer('relu4_3', net['conv4_3'], b=get_bias(vgg_layers, 23))abcdefabcdefzzzzzznet['conv4_4'] = conv_layer('conv4_4', net['relu4_3'], W=get_weights(vgg_layers, 25))abcdefzzzzzznet['relu4_4'] = relu_layer('relu4_4', net['conv4_4'], b=get_bias(vgg_layers, 25))abcdefabcdefzzzzzznet['pool4']zzz = pool_layer('pool4', net['relu4_4'])abcdefabcdefzzzzzz# if args.verbose: print('LAYER GROUP 5')abcdefzzzzzznet['conv5_1'] = conv_layer('conv5_1', net['pool4'], W=get_weights(vgg_layers, 28))abcdefzzzzzznet['relu5_1'] = relu_layer('relu5_1', net['conv5_1'], b=get_bias(vgg_layers, 28))abcdefabcdefzzzif layer_num == 5:abcdefzzzzzznet['conv5_1'] = conv_layer('conv5_1', net['input'], W=get_weights(vgg_layers, 28))abcdefzzzzzznet['relu5_1'] = relu_layer('relu5_1', net['conv5_1'], b=get_bias(vgg_layers, 28))abcdefabcdefzzzzzznet['conv5_2'] = conv_layer('conv5_2', net['relu5_1'], W=get_weights(vgg_layers, 30))abcdefzzzzzznet['relu5_2'] = relu_layer('relu5_2', net['conv5_2'], b=get_bias(vgg_layers, 30))abcdefabcdefzzzzzznet['conv5_3'] = conv_layer('conv5_3', net['relu5_2'], W=get_weights(vgg_layers, 32))abcdefzzzzzznet['relu5_3'] = relu_layer('relu5_3', net['conv5_3'], b=get_bias(vgg_layers, 32))abcdefabcdefzzzzzznet['conv5_4'] = conv_layer('conv5_4', net['relu5_3'], W=get_weights(vgg_layers, 34))abcdefzzzzzznet['relu5_4'] = relu_layer('relu5_4', net['conv5_4'], b=get_bias(vgg_layers, 34))abcdefabcdefzzzzzznet['pool5']zzz = pool_layer('pool5', net['relu5_4'])abcdefabcdefzzzreturn netabcdefabcdefabcdefabcdefabcdefdef createImage(tensor):abcdefzzzzzznewImage = np.zeros((len(tensor[0]),len(tensor[0][0]),3))abcdefzzzzzzfor z in range(len(tensor[0][0][0])):abcdefzzzzzzzzzzzzfor y in range(len(tensor[0])):abcdefzzzzzzzzzzzzzzzfor x in range(len(tensor[0][y])):abcdefzzzzzzzzzzzzzzzzzznewImage[y][x][0] = tensor[0][y][x][z]abcdefzzzzzzzzzzzzzzzzzznewImage[y][x][1] = tensor[0][y][x][z]abcdefzzzzzzzzzzzzzzzzzznewImage[y][x][2] = tensor[0][y][x][z]abcdefzzzzzzzzzzzznewImage = np.asarray(newImage).astype('float64')abcdefzzzzzzzzzzzznewImage += 127.0#MEAN_VALUES[0]abcdefzzzzzzzzzzzznewImage = newImage[:, :, ::-1]abcdefzzzzzzzzzzzznewImage = np.clip(newImage, 0, 255).astype('uint8')abcdefzzzzzzprint(newImage.shape)abcdefzzzzzzreturn newImageabcdefabcdefabcdefdef minimize_with_lbfgs(sess, net, optimizer, init_img, goal_img):abcdefzzzinit_op = tf.global_variables_initializer()abcdefzzzsess.run(init_op)abcdefzzzsess.run(net['input'].assign(init_img))abcdefzzz# sess.run(net['y_'].assign(goal_img))abcdefzzzoptimizer.minimize(sess)abcdefabcdefdef minimize_with_adam(sess, net, optimizer, init_img, goal_img, loss, max_iterations, check_layer, output_layer, print_iterations):abcdefzzztrain_op = optimizer.minimize(loss)abcdefzzzinit_op = tf.global_variables_initializer()abcdefzzzsess.run(init_op)abcdefabcdefzzziterations = 0abcdefzzzwhile (iterations < max_iterations):abcdefzzzzzzsess.run(train_op)abcdefzzzzzzif iterations % print_iterations==0:abcdefzzzzzzzzzcurr_loss = loss.eval()abcdefzzzzzzzzzprint("At iterate {}\tf=zzz{:.5E}".format(iterations, curr_loss))abcdefzzzzzzzzz# cv2.imwrite("./output/"+str(iterations)+"_input.jpg",createImage(sess.run(net[output_layer])))abcdefzzzzzzzzz# cv2.imwrite("./output/"+str(iterations)+"_relu.jpg",createImage(sess.run(net[check_layer])))abcdefabcdefzzzzzziterations += 1abcdefabcdefabcdefdef getLoss(sess, vggModel, layer, generate_image, goal_image):abcdefabcdefzzzzzzsess.run(vggModel['input'].assign(generate_image))abcdefabcdefzzzzzzsub = tf.subtract(vggModel[layer],vggModel['y_'])abcdefzzzzzzabso= tf.norm(sub)abcdefzzzzzzreturn tf.pow(abso,2)abcdefabcdefabcdefdef get_optimizer(loss, select_optimizer, learning_rate, max_iterations, print_iterations):abcdefzzzif select_optimizer == 'lbfgs':abcdefzzzzzzoptimizer = tf.contrib.opt.ScipyOptimizerInterface(abcdefzzzzzzzzzloss, method='L-BFGS-B',abcdefzzzzzzzzzoptions={'maxiter': max_iterationsabcdefzzzzzzzzzzzzzzzzzzzzzzzzzzz})abcdefzzzelif select_optimizer == 'adam':abcdefzzzzzzoptimizer = tf.train.AdamOptimizer(learning_rate)abcdefzzzreturn optimizerabcdefabcdefabcdefabcdefdef newDeconv(sess, goal_img, select_optimizer, max_iterations, check_layer, output_layer, layer_num):abcdefzzzzzz# vggModel['y_'] = tf.Variable(tf.constant(goal_img))abcdefzzzzzznoise = generateNoiseImage(int(len(goal_img[0])*2),int(len(goal_img[0][0])*2),128)abcdefzzzzzznoise = np.asarray(noise,dtype=np.float32)abcdefzzzzzznoise -= 127.0abcdefzzzzzzinit_img = np.asarray([noise],dtype=np.float32)abcdefabcdefzzzzzzgoal_img -=127.0abcdefzzzzzzgoal_img = np.asarray([goal_img],dtype=np.float32)abcdefabcdefabcdefzzzzzz# noise -= 127.0#MEAN_VALUES[0]abcdefzzzzzzvggModel = build_model(noise,int(len(goal_img[0])*2),int(len(goal_img[0][0])*2),128,layer_num)abcdefzzzzzzvggModel['y_'] = tf.constant(goal_img)abcdefabcdefzzzzzzL_total=getLoss(sess, vggModel, check_layer, init_img, goal_img)abcdefzzzzzz# L_total = sum_total_variation_losses(sess, vggModel, init_img)abcdefzzzzzzoptimizer=get_optimizer(L_total,select_optimizer, 1e-1, max_iterations,1000)abcdefzzzzzzif select_optimizer == 'adam':abcdefzzzzzzzzzzzzminimize_with_adam(sess, vggModel, optimizer, init_img, goal_img, L_total,max_iterations,1000)abcdefzzzzzzelif select_optimizer == 'lbfgs':abcdefzzzzzzzzzzzzminimize_with_lbfgs(sess, vggModel, optimizer, init_img, goal_img)abcdefabcdefzzzzzzreturn sess.run(vggModel[output_layer])abcdefzzzzzz# output_img = sess.run(vggModel[check_layer])abcdefzzzzzz# in_img = sess.run(vggModel[output_layer])abcdefabcdefzzzzzz# print(output_img.shape)abcdefzzzzzz# output_img = createImage(output_img)abcdefzzzzzz# print(output_img.shape)abcdefzzzzzz# output_img = createImage(output_img)# + MEAN_VALUESabcdefzzzzzz# output_img = np.clip(output_img,0,255).astype('uint8')abcdefzzzzzz# cv2.imshow("L4",output_img)abcdefzzzzzz# cv2.waitKey(0)abcdefzzzzzz# cv2.imshow("L3_2",createImage(in_img))abcdefzzzzzz# cv2.waitKey(0)abcdefzzzzzz# cv2.imshow("input",createImage(sess.run(vggModel['input'])))abcdefzzzzzz# cv2.waitKey(0)abcdefabcdefdef generateNoiseImage(height, width, channel):abcdefzzzzzzrandomByteArray = bytearray(os.urandom(height*width*channel)) #画素数文の乱数発生abcdefzzzzzzreturn np.asarray(randomByteArray).reshape((height, width, channel))abcdefabcdefdef getMono(img):abcdefzzzzzzreturn cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)abcdefabcdefabcdefabcdefabcdefsess = tf.InteractiveSession()abcdeftf.global_variables_initializer().run()abcdef# model = load_vgg_model(VGG_MODEL,len(PhiAB[0]),len(PhiAB[0][0]),512)abcdefabcdef# img = cv2.imread('avater.jpg')abcdef# img = np.asarray(img).astype('float64')abcdefabcdef# img -= MEAN_VALUES[0]abcdef# img = img[:, :, ::-1].astype('float64')abcdefabcdefabcdef# # img = img.transpose((2, 0, 1))abcdef# img = np.expand_dims(img, axis=0)abcdefabcdef# sess.run(model['input'].assign(np.asarray(img)))abcdef# A=[]abcdef# A.append(createImage(sess.run(model['conv1_1'])))abcdef# A.append(createImage(sess.run(model['conv2_1'])))abcdef# A.append(createImage(sess.run(model['conv3_1'])))abcdef# A.append(createImage(sess.run(model['conv4_1'])))abcdef# A.append(createImage(sess.run(model['conv5_1'])))abcdefabcdef# PhiAB =getPhi_Random(A[5-1])abcdef# noise = generateNoiseImage(IMAGE_HEIGHT,IMAGE_WIDTH,3)abcdef# cv2.imshow("",noise)abcdef# cv2.waitKey(0)abcdef# PhiAB = getMono(PhiAB)abcdef# noise = getMono(noise)abcdef# cv2.imshow("",warp(A[5-1],PhiAB))abcdef# cv2.waitKey(0)abcdef# sess, generate_image, goal_image, channel, select_optimizer, max_iterationsabcdef# deconv(sess, np.asarray([noise],dtype=np.float32), np.asarray([PhiAB],dtype=np.float32),3,'adam', 20000)abcdef# deconv(noise, PhiAB, 3, 20000)abcdefPhiAB = cv2.imread("L4.jpg",cv2.IMREAD_GRAYSCALE)abcdefabcdefgoal = np.zeros((len(PhiAB),len(PhiAB[0]),512))abcdeffor y in range(len(PhiAB)):abcdefzzzzzzfor x in range(len(PhiAB[y])):abcdefzzzzzzzzzzzzfor z in range(512):abcdefzzzzzzzzzzzzzzzzzzgoal[y][x][z] = PhiAB[y][x]abcdefabcdef# goal = np.asarray(goal,dtype='float32')abcdefabcdefabcdefabcdefabcdefabcdefabcdef# graph = TrueDeconvGrapf(np.asarray([noise],dtype=np.float32), np.asarray([PhiAB],dtype=np.float32), 3)abcdeftf.global_variables_initializer().run()abcdefabcdefabcdefimg = newDeconv(sess,zzzgoal,'adam', 200001,3)abcdefsess.close()abcdef
